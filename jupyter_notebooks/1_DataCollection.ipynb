{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f14624a",
   "metadata": {},
   "source": [
    "# **TradeCare: Data Collection Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b7702",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* Fetch historical Bitcoin OHLCV (Open, High, Low, Close, Volume) data from a GitHub-hosted repository that provides automated daily updates.\n",
    "* Verify data loaded correctly (basic checks)\n",
    "* Understand data structure and characteristics\n",
    "* Document data source and live data collection strategy\n",
    "\n",
    "## Inputs\n",
    "*  **Data Source:** GitHub Repository (automated updates)\n",
    "*   **URL:** https://raw.githubusercontent.com/mouadja02/bitcoin-hourly-ohclv-dataset/main/btc-hourly-price_2015_2025.csv\\n\n",
    "*   **Asset:** BTC-USD\n",
    "*   **Timeframe:** 1 Hour\n",
    "*   **Period:** November 2014 - present\n",
    "\n",
    "## Outputs\n",
    "* DataFrame loaded in memory for exploration\n",
    "* Data understanding documented\n",
    "* Validated raw data saved as CSV checkpoint: `inputs/datasets/raw/bitcoin_raw.csv`\n",
    "* Subsequent notebooks load from CSV\n",
    "\n",
    "\n",
    "## Additional Comments\n",
    "This GitHub dataset provides a **unique combination** rarely found in ML projects:\n",
    "\n",
    "* **Fresh & Maintained:** Automated workflow fetches current data from CryptoCompare API daily and stores backups on GitHub. Repository contains Bitcoin hourly price data from 2015 to present with continuous updates\n",
    "* **Simple**: Direct CSV access via single URL\n",
    "* **Free**: No API keys or costs  \n",
    "* **Reliable**: No rate limits or auth failures  \n",
    "* **Transparent**: Git history shows every change  \n",
    "* **Scalable**: Should work in production environments  \n",
    "\n",
    "**Data Pipeline Strategy:**\n",
    "* This notebook fetches data from URL and validates it\n",
    "* Validated data is saved as CSV checkpoint for fast iteration\n",
    "* Subsequent notebooks load from CSV (no re-fetching needed)\n",
    "* This provides: reliability, speed, and offline capability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21778133",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ca8ca",
   "metadata": {},
   "source": [
    "## Change Working Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2f33d",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with `os.getcwd()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a21e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd609a",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e01b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3793eff",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09525469",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb167f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2a604",
   "metadata": {},
   "source": [
    "# Fetch and Validate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824cfea",
   "metadata": {},
   "source": [
    "## Import Validation Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import centralized validation function\n",
    "import sys\n",
    "sys.path.append('.')  # Add project root to path\n",
    "\n",
    "from src.raw_data_validation import fetch_and_validate_data, get_data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8295c97",
   "metadata": {},
   "source": [
    "## Fetch Validated Data\n",
    "\n",
    "This function automatically:\n",
    "- Fetches data from GitHub URL\n",
    "- Validates column structure and names\n",
    "- Validates string data safety (no injection attempts)\n",
    "- Validates price ranges\n",
    "- Validates data completeness\n",
    "- Validates timestamps\n",
    "\n",
    "If any validation fails, the notebook stops with a clear error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9eaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and validate data in one call\n",
    "df = fetch_and_validate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28991075",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d362e5c",
   "metadata": {},
   "source": [
    "# Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data summary from helper function\n",
    "import json\n",
    "import pandas as pd\n",
    "data_info = get_data_info(df)\n",
    "print(json.dumps(data_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c138b9ed",
   "metadata": {},
   "source": [
    "## DataFrame Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data fetched on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total rows: {df.shape[0]:,}\")\n",
    "print(f\"Total columns: {df.shape[1]}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25167701",
   "metadata": {},
   "source": [
    "## Display First Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb82789",
   "metadata": {},
   "source": [
    "## Display Last Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69d5eb",
   "metadata": {},
   "source": [
    "## DataFrame Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e87af",
   "metadata": {},
   "source": [
    "## Statistical Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f7611",
   "metadata": {},
   "source": [
    "NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cf58b",
   "metadata": {},
   "source": [
    "* You may add as many sections as you want, as long as it supports your project workflow.\n",
    "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eecfbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1397464",
   "metadata": {},
   "source": [
    "# Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed4f3f",
   "metadata": {},
   "source": [
    "## Check for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89193e86",
   "metadata": {},
   "source": [
    "## Check for Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eaf3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    print(df[df.duplicated(keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab5ca7",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31346e",
   "metadata": {},
   "source": [
    "# Save Validated Data Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "raw_data_dir = 'inputs/datasets/raw'\n",
    "os.makedirs(raw_data_dir, exist_ok=True)\n",
    "print(f\"✓ Directory created/verified: {raw_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f3ded",
   "metadata": {},
   "source": [
    "## Save Raw Data as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validated data\n",
    "csv_path = f\"{raw_data_dir}/bitcoin_raw.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Confirm save\n",
    "file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "print(f\"✓ Data saved successfully\")\n",
    "print(f\"  Location: {csv_path}\")\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "print(f\"  Timestamp: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb6028",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42783b",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e8683",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ **Data Collection Complete**\n",
    "\n",
    "This notebook successfully:\n",
    "1. Used centralized validation helper (`src/raw_data_validation.py`)\n",
    "2. Fetched Bitcoin hourly OHLCV data from GitHub repository\n",
    "3. Automatically validated data structure, safety, and integrity\n",
    "4. Explored the validated dataset\n",
    "\n",
    "**Security Measures Applied:**\n",
    "- Column structure verification\n",
    "- Character injection prevention (no dangerous symbols)\n",
    "- Date/time format validation\n",
    "- Price range sanity checks\n",
    "- Data completeness validation\n",
    "- Timestamp range verification\n",
    "\n",
    "**Key Findings:**\n",
    "- Data covers the period from November 2014 to present (November 2025)\n",
    "- Hourly granularity provides sufficient detail for short-term predictions\n",
    "- All security validations passed\n",
    "- **Dataset is exceptionally clean:**\n",
    "  - No missing values detected\n",
    "  - No duplicate rows found\n",
    "  - Automated data collection ensures consistency\n",
    "  - Public API source reduces manual entry errors\n",
    "  - Validation confirms data integrity\n",
    "\n",
    "**Data Quality Notes:**\n",
    "- This dataset benefits from automated collection via CryptoCompare API\n",
    "- Programmatic data generation minimizes human input errors\n",
    "- Continuous validation by repository maintainers ensures reliability\n",
    "- However, cleaning pipeline will be implemented for:\n",
    "  - Future-proofing against potential data gaps\n",
    "  - Demonstrating data preparation best practices\n",
    "  - Handling edge cases in production deployment\n",
    "\n",
    "**Data Pipeline Approach:**\n",
    "- Validated raw data saved to: `inputs/datasets/raw/bitcoin_raw.csv`\n",
    "- Subsequent notebooks will load from CSV (fast, reliable)\n",
    "- Data cleaning notebook with minimal intervention needed\n",
    "- Feature engineering will transform raw data to ML-ready format\n",
    "\n",
    "**Next Steps:**\n",
    "1. Proceed to Data Cleaning notebook (`2_DataCleaning.ipynb`)\n",
    "2. Load raw CSV and confirm data quality\n",
    "4. Prepare for feature engineering phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f5fa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90aa00f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
